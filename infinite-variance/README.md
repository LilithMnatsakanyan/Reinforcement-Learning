# **Infinite Variance in Off-Policy Evaluation**

This project explores the phenomenon of **infinite variance** in **Monte Carlo off-policy evaluation** using **importance sampling**, based on **Chapter 5: Monte Carlo Methods** from *Reinforcement Learning: An Introduction* by **Richard S. Sutton & Andrew G. Barto**.

---

## ğŸ“– References
This repository is based on:

- **Reinforcement Learning: An Introduction**  
  **Richard S. Sutton & Andrew G. Barto**  
  _Second Edition, MIT Press, 2018_  
  [Read online](https://www.andrew.cmu.edu/course/10-703/textbook/BartoSutton.pdf)

---

## ğŸ“‚ Project Structure
```
infinite-variance/
â”‚â”€â”€ src/                            # Core simulation code
â”‚   â”œâ”€â”€ infinite_variance.py
â”‚â”€â”€ notebooks/                      # Jupyter Notebook for experiments
â”‚   â”œâ”€â”€ infinite_variance.ipynb
â”‚â”€â”€ book_images/                    # Figures from Sutton & Barto
â”‚   â”œâ”€â”€ Figure_5_4_1.PNG
â”‚   â”œâ”€â”€ Figure_5_4_2.PNG
â”‚â”€â”€ generated_images/              # Generated result plots
â”‚   â”œâ”€â”€ figure_5_4.png
â”‚â”€â”€ README.md                      # Project documentation
```

---

## ğŸ“Œ Key Features

âœ… Simulates **off-policy evaluation** via **importance sampling**  
âœ… Uses a custom environment with **rare target trajectories**  
âœ… Defines clear **target** and **behavior** policies  
âœ… Reveals how **importance ratios explode** in long episodes  
âœ… Includes visual comparisons with **book figures** and **simulation output**

---

## âš™ï¸ Environment Description

This environment demonstrates how importance sampling can lead to extreme variance when the behavior and target policies diverge significantly.

- **Actions:** `left` (0) and `right` (1)
- **Target Policy:** Always selects `left`
- **Behavior Policy:** Randomly selects `left` or `right` with equal probability
- **Reward Structure:**
  - Choosing `right` ends the episode with reward = 0
  - Choosing `left` continues the episode with 90% probability, else ends with reward = 1

This structure means the probability of an episode strictly following the target policy drops exponentially with episode length, resulting in **massive importance weights** during evaluation.

---

## ğŸ“Š Results and Visualizations

### 1ï¸âƒ£ **Figures from Sutton's Book**
These reference figures illustrate the instability in off-policy Monte Carlo evaluation.

ğŸ“ˆ **Book Visuals:**

![Figure_5_4_1.PNG](book_images/Figure_5_4_1.PNG)

![Figure_5_4_2.PNG](book_images/Figure_5_4_2.PNG)
---

### 2ï¸âƒ£ **Generated Simulation Output**
The simulation output replicates the conditions discussed in the book, showing highly unstable value estimates due to exploding variance.

ğŸ“ˆ **Generated Plot:**

![figure_5_4.png](generated_images/figure_5_4.png)
---

## ğŸ” Interpretation of Results

This example clearly shows how importance sampling estimators can be **unbiased but unstable**. In particular:

- Most episodes terminate early due to random `right` selections.
- Rare episodes fully match the target policy, producing **huge importance weights**.
- The estimates swing wildly, demonstrating **high and persistent variance**.

These outcomes support Sutton & Bartoâ€™s discussion that ordinary importance sampling can have **infinite variance**, and that more samples **do not guarantee stability**.

---

## ğŸ“¢ Conclusion

This project illustrates a core challenge in **off-policy learning**:

- **Ordinary importance sampling** can be **theoretically correct** but practically **unusable** due to infinite variance.
- Shows why **weighted importance sampling**, bootstrapping, or variance-reducing techniques are often essential.

By simulating this failure case, we build deeper understanding of when and why Monte Carlo methods may break downâ€”reinforcing the importance of careful algorithm design in reinforcement learning.

---

