# **Reinforcement Learning: n-step TD on Random Walk**

This project implements **n-step Temporal Difference (TD) methods** for solving the **Random Walk** problem.  
It is based on **Chapter 7: n-step Bootstrapping**, specifically **Example 7.2**, from the book *Reinforcement Learning: An Introduction* by **Richard S. Sutton & Andrew G. Barto**.

---

## **ğŸ“‚ Project Structure**

```
random-walk-ntd/
â”œâ”€â”€ src/ # Core implementation
â”‚ â””â”€â”€ random_walk.py # Logic for n-step TD methods
â”œâ”€â”€ notebooks/ # Jupyter Notebook for experimentation
â”‚ â””â”€â”€ random_walk.ipynb
â”œâ”€â”€ book_images/ # Reference images from the book
â”‚ â”œâ”€â”€ Example_6_2_top.PNG
â”‚ â””â”€â”€ Figure_7_2.PNG
â”œâ”€â”€ generated_images/ # Plots generated from simulations
â”‚ â””â”€â”€ figure_7_2.png
â””â”€â”€ README.md # Project documentation
```

---

## ğŸ“Œ Key Features
âœ… Implements **n-step TD prediction** (generalizing TD(0) and Monte Carlo)  
âœ… Uses **19-state Random Walk** environment with terminal rewards  
âœ… Compares learning performance for different **n-step values**  
âœ… Reproduces Suttonâ€™s **Figure 7.2** experiment  
âœ… Modular implementation for experimentation in Python or Jupyter

---

## âš™ï¸ **Environment Overview**

- A **1D Random Walk** with **19 non-terminal states**:
    - Terminal states: `State 0 (Left)` and `State 20 (Right)`
    - Non-terminal states: `States 1â€“19`
- The agent always starts from **State 10 (middle)**.
- At each step, the agent randomly moves **left or right** with equal probability (p=0.5).
- Rewards:
    - Left terminal (`State 0`) â†’ **-1**
    - Right terminal (`State 20`) â†’ **+1**
    - All other states â†’ **0**

### âœ… **True State Values**
These represent the ground truth (linear ramp between -1 and +1):

```V(s) = -0.9, -0.8, ..., 0.8, 0.9```   for states 1â€“19

---

## ğŸ§  **Learning Algorithm: n-step TD**

### ğŸ”· n-step TD Prediction
- Generalizes between **TD(0)** and **Monte Carlo**:
    - `n = 1` â†’ equivalent to **TD(0)**
    - `n â†’ âˆ` â†’ approaches **Monte Carlo**
- Update rule:  
  ```G = R(t+1) + Î³ * R(t+2) + Î³^2 * R(t+3) + ... + Î³^(n-1) * R(t+n) + Î³^n * V(S(t+n))```
- Balances **bias and variance** depending on `n`.

---

## ğŸ“Š Results and Visualizations

### 1ï¸âƒ£ **Figures from Suttonâ€™s Book**
These show how n-step TD methods converge and compare:

ğŸ“ˆ **Visualization:**  
<img src="book_images/Figure_7_2.PNG" alt="Sutton Fig 7.2" width="400"/>  
<img src="book_images/Example_6_2_top.PNG" alt="Reference Example" width="400"/>

---

### 2ï¸âƒ£ **Generated Simulation Results**
Simulation outputs from this implementation:

ğŸ“ˆ **Visualization:**  
<img src="generated_images/figure_7_2.png" alt="n-step TD Results" width="400"/>

These demonstrate how varying **n** affects convergence speed and accuracy.

---

## ğŸ” **Key Observations**
- **Smaller n (e.g., 1-step TD)** â†’ more biased but faster updates.
- **Larger n** â†’ less bias, higher variance.
- Intermediate values of `n` provide a **tradeoff** between speed and stability.
- Reproduces Suttonâ€™s results, validating the implementation.

---

## ğŸ“¢ Conclusion
This project demonstrates:

- How **n-step TD** bridges the gap between **TD(0)** and **Monte Carlo**.
- The impact of **n** on learning speed, stability, and variance.
- A faithful reproduction of **Suttonâ€™s Example 7.2** with hands-on code.

The Random Walk with n-step TD is a fundamental experiment for understanding **bootstrapping, variance, and bias tradeoffs** in reinforcement learning.  
