# **Reinforcement Learning: Cliff Walking**

This project implements **SARSA**, **Expected SARSA**, and **Q-Learning** for solving the **Cliff Walking** environment, based on **Chapter 6: Temporal-Difference Learning** from the book **Reinforcement Learning: An Introduction** by **Richard S. Sutton & Andrew G. Barto**.

---

## **ğŸ“‚ Project Structure**
```
cliff-walking/
â”‚â”€â”€ .idea/                         # Project settings
â”‚â”€â”€ book_images/                   # Reference figures from the book
â”‚   â”œâ”€â”€ Example_6_6_graph.PNG
â”‚   â”œâ”€â”€ Example_6_6_grid.PNG
â”‚   â”œâ”€â”€ Figure_6_3.PNG
â”‚â”€â”€ generated_images/              # Generated visualizations
â”‚   â”œâ”€â”€ example_6_6.png
â”‚   â”œâ”€â”€ figure_6_3.png
â”‚â”€â”€ notebooks/                     # Jupyter Notebook for experiments
â”‚   â”œâ”€â”€ cliff_walking.ipynb
â”‚â”€â”€ src/                           # Source code
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ cliff_walking.py
â”‚â”€â”€ README.md                      # Project documentation
```

---

## ğŸ“Œ Key Features

âœ… Implements **SARSA**, **Expected SARSA**, and **Q-Learning** algorithms  
âœ… Uses an **Îµ-greedy** policy for exploration  
âœ… Handles **cliff transitions** with severe penalties  
âœ… Demonstrates the trade-off between **exploration vs. exploitation**  
âœ… Includes **optimal policy printing** for visual interpretation

---

## ğŸ“Š Results and Visualizations

### 1ï¸âƒ£ **Figures from Sutton's Book**
These figures illustrate the Cliff Walking task and the comparison between SARSA and Q-learning methods.

ğŸ“ˆ **Visualization:**

<img src="book_images/Example_6_6_grid.PNG" alt="Cliff Grid" width="400"/>
<img src="book_images/Example_6_6_graph.PNG" alt="SARSA vs Q-Learning" width="400"/>
<img src="book_images/Figure_6_3.PNG" alt="TD Learning Summary" width="400"/>

---

### 2ï¸âƒ£ **Generated Simulation Results**
These are visualizations produced from this project showing learned policies and performance comparison:

ğŸ“ˆ **Visualization:**

<img src="generated_images/example_6_6.png" alt="SARSA Learned Policy" width="400"/>
<img src="generated_images/figure_6_3.png" alt="Q-Learning vs SARSA" width="400"/>

---

## ğŸ” Interpretation of Results

- **SARSA** learns a conservative path, avoiding the cliff to minimize risk and long-term penalty.  
- **Q-Learning**, being off-policy, tends to learn the shortest risky path, falling off the cliff more frequently.  
- **Expected SARSA** combines both and usually performs more stably across episodes.  
- This difference in learned behavior reflects the **on-policy vs off-policy** learning nature.

---

## ğŸ“¢ Conclusion

This project demonstrates the core ideas of **Temporal-Difference Learning**:

- Learning value functions from raw experience without models  
- Exploring **SARSA**, **Expected SARSA**, and **Q-Learning** differences  
- Understanding the role of **step-size**, **exploration**, and **update strategy**

It gives powerful insights into how agents learn under risk and uncertainty in reinforcement learning.

---
