{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# 10-armed Testbed"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "cada6500ddd403c4"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "d2b19d40efb1b436"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class Bandit:\n",
    "    # region Constructor\n",
    "\n",
    "    def __init__(self, arms_number: int = 10, use_sample_averages: bool = False, epsilon=0., initial_action_value_estimates=0., confidence_level=None,\n",
    "                 use_gradient: bool = False, step_size=0.1, use_gradient_baseline: bool = False, true_expected_reward=0.):\n",
    "        # region Summary\n",
    "        \"\"\"\n",
    "        k-armed Bandit.\n",
    "        :param arms_number: (denoted as k) number of bandit's arms\n",
    "        :param use_sample_averages: if True, use sample-average method for estimating action values\n",
    "        :param epsilon: (denoted as ε) probability for exploration in ε-greedy algorithm\n",
    "        :param initial_action_value_estimates: (denoted as 𝑄_1(𝑎)) initial estimation for each action value\n",
    "        :param confidence_level: (denoted as 𝑐) if not None, use Upper-Confidence-Bound (UCB) action selection\n",
    "        :param use_gradient: if True, use Gradient Bandit Algorithm (GBA)\n",
    "        :param step_size: (denoted as 𝛼) constant step size for updating estimates\n",
    "        :param use_gradient_baseline: if True, use average reward as baseline for GBA\n",
    "        :param true_expected_reward: true expected rewards selected from normal (Gaussian) distribution with μ=4 mean and σ=1 variance\n",
    "        \"\"\"\n",
    "        # endregion Summary\n",
    "\n",
    "        # region Body\n",
    "\n",
    "        self.k = arms_number\n",
    "        self.actions = np.arange(self.k)\n",
    "\n",
    "        # Value of each action is expected or mean reward given that that action is selected (denoted as 𝑞_∗(𝑎))\n",
    "        self.action_values = None\n",
    "\n",
    "        # Estimated value of each action (denoted as 𝑄_𝑡(𝑎))\n",
    "        self.estimated_action_values = None\n",
    "\n",
    "        # region Action-Value Methods\n",
    "\n",
    "        # region Sample-average Method\n",
    "\n",
    "        self.use_sample_averages = use_sample_averages\n",
    "\n",
    "        # endregion Sample-average Method\n",
    "\n",
    "        # region Action Selection Methods\n",
    "\n",
    "        # region ε-greedy\n",
    "\n",
    "        self.epsilon = epsilon\n",
    "\n",
    "        # endregion ε-greedy\n",
    "\n",
    "        # region Optimistic Initial Values\n",
    "\n",
    "        self.initial_action_value_estimates = initial_action_value_estimates\n",
    "\n",
    "        # endregion Optimistic Initial Values\n",
    "\n",
    "        # region UCB\n",
    "\n",
    "        self.confidence_level = confidence_level\n",
    "\n",
    "        # Time steps\n",
    "        self.time = 0\n",
    "\n",
    "        # Number of times each action has been selected (denoted as 𝑁_𝑡(𝑎))\n",
    "        self.action_selection_count = None\n",
    "\n",
    "        # endregion UCB\n",
    "\n",
    "        # region GBA\n",
    "\n",
    "        self.use_gradient = use_gradient\n",
    "\n",
    "        # Probability of taking action 𝑎 at time 𝑡 (denoted as 𝜋_𝑡(𝑎))\n",
    "        self.action_probability = None\n",
    "\n",
    "        self.step_size = step_size\n",
    "\n",
    "        # Average of the rewards up to (but not including) time 𝑡 (denoted as 𝑅̅_𝑡)\n",
    "        self.average_reward = 0\n",
    "\n",
    "        self.use_gradient_baseline = use_gradient_baseline\n",
    "\n",
    "        self.true_expected_reward = true_expected_reward\n",
    "\n",
    "        # endregion GBA\n",
    "\n",
    "        # endregion Action Selection Methods\n",
    "\n",
    "        # endregion Action-Value Methods\n",
    "\n",
    "        # Optimal action\n",
    "        self.optimal_action = None\n",
    "\n",
    "        # endregion Body\n",
    "\n",
    "    # endregion Constructor\n",
    "\n",
    "    # region Functions\n",
    "\n",
    "    def initialize(self):\n",
    "        # region Summary\n",
    "        \"\"\"\n",
    "        Initialize action parameters\n",
    "        \"\"\"\n",
    "        # endregion Summary\n",
    "\n",
    "        # region Body\n",
    "\n",
    "        # Initialize action values according to a normal (Gaussian) distribution with μ=0 mean and σ=1 variance.\n",
    "        # In case of GBA, add true_expected_reward != 0.\n",
    "        self.action_values = np.random.randn(self.k) + self.true_expected_reward\n",
    "\n",
    "        # In case of realistic initial values, initialize estimated action values with 0s.\n",
    "        # In case of optimistic initial values, add initial_action_value_estimates != 0\n",
    "        self.estimated_action_values = np.zeros(self.k) + self.initial_action_value_estimates\n",
    "\n",
    "        # Set time steps to 0\n",
    "        self.time = 0\n",
    "\n",
    "        # Initialize number of times each action has been selected to 0 (none of actions has been selected yet)\n",
    "        self.action_selection_count = np.zeros(self.k)\n",
    "\n",
    "        # Optimal action is the action with the highest value\n",
    "        self.optimal_action = np.argmax(self.action_values)\n",
    "\n",
    "        # endregion Body\n",
    "\n",
    "    def act(self):\n",
    "        # region Summary\n",
    "        \"\"\"\n",
    "        Get an action for this bandit.\n",
    "        :return: Action\n",
    "        \"\"\"\n",
    "        # endregion Summary\n",
    "\n",
    "        # region Body\n",
    "\n",
    "        # region ε-greedy\n",
    "\n",
    "        # ε-greedy action selection: every once in a while, with small probability ε, select randomly from among all the actions with equal probability, independently of the action-value estimates.\n",
    "        if np.random.rand() < self.epsilon:\n",
    "            return np.random.choice(self.actions)\n",
    "\n",
    "        # endregion ε-greedy\n",
    "\n",
    "        # region Greedy\n",
    "\n",
    "        # Greedy action selection: select one of the actions with the highest estimated value, that is, one of the greedy actions.\n",
    "        # If there is more than one greedy action, then a selection is made among them in some arbitrary way, perhaps randomly.\n",
    "        action = np.random.choice(np.where(self.estimated_action_values == np.max(self.estimated_action_values))[0])\n",
    "        return action\n",
    "        # endregion Greedy\n",
    "\n",
    "        # endregion Body\n",
    "\n",
    "    def step(self, action):\n",
    "        # region Summary\n",
    "        \"\"\"\n",
    "        Update estimated action value and return reward for this action.\n",
    "        :param action: Action\n",
    "        :return: Reward\n",
    "        \"\"\"\n",
    "        # endregion Summary\n",
    "\n",
    "        # region Body\n",
    "\n",
    "        # When a learning method applied to that bandit problem selected action 𝐴_𝑡 at time step 𝑡, the actual reward, 𝑅_𝑡, was selected from\n",
    "        # a normal (Gaussian) distribution with μ = 𝑞_∗(𝑎) mean and σ = 1 variance\n",
    "        actual_reward = np.random.randn() + self.action_values[action]\n",
    "\n",
    "        # Add 1 to time step\n",
    "        self.time += 1\n",
    "\n",
    "        # Add 1 to number of times this action has been selected\n",
    "        self.action_selection_count[action] += 1\n",
    "\n",
    "        # The average of the rewards can be computed incrementally\n",
    "        # The Bandit Gradient Algorithm as Stochastic Gradient Ascent\n",
    "        self.average_reward += (actual_reward - self.average_reward) / self.time\n",
    "\n",
    "        if self.use_sample_averages: # Update estimated action values using sample-average method\n",
    "            # Incremental Implementation (Equation 2.3)\n",
    "            self.estimated_action_values[action] += (actual_reward - self.estimated_action_values[action]) / self.action_selection_count[action]\n",
    "\n",
    "        elif self.use_gradient: # Update estimated action values using GBA\n",
    "            one_hot_encoding = np.zeros(self.k)\n",
    "            one_hot_encoding[action] = 1\n",
    "\n",
    "            # The average of the rewards can serve as a baseline with which the reward is compared.\n",
    "            baseline = self.average_reward if self.use_gradient_baseline else 0\n",
    "\n",
    "            # A natural learning algorithm for soft-max action preferences based on the idea of stochastic gradient ascent:\n",
    "            # on each step, after selecting action 𝐴_𝑡 and receiving the reward 𝑅_𝑡, the action preferences are updated by Equation 2.12:\n",
    "            self.estimated_action_values += self.step_size * (actual_reward - baseline) * (one_hot_encoding - self.action_probability)\n",
    "\n",
    "        else: # Update estimated action values with constant step size\n",
    "            # Incremental Implementation (Equation 2.3) with constant step size parameter\n",
    "            self.estimated_action_values[action] += self.step_size * (actual_reward - self.estimated_action_values[action])\n",
    "\n",
    "        return actual_reward\n",
    "\n",
    "        # endregion Body\n",
    "\n",
    "    # endregion Functions\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-02-24T10:12:37.933565Z",
     "start_time": "2025-02-24T10:12:37.923822Z"
    }
   },
   "id": "9589c2cfb46bc952",
   "execution_count": 37
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-02-24T10:12:39.093015Z",
     "start_time": "2025-02-24T10:12:39.088884Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tqdm import trange\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "sys.path.append(r\"C:\\Users\\lilit\\PycharmProjects\\Reinforcement_Learning\\Ten-Armed-Testbed\")\n",
    "\n",
    "# from src.bandit import Bandit\n",
    "\n",
    "\n",
    "matplotlib.use('Agg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "outputs": [],
   "source": [
    "def simulate(runs, times, bandits):\n",
    "    # region Summary\n",
    "    \"\"\"\n",
    "    For any learning method, we can measure its performance and behavior as it improves with experience over 1000 time steps \n",
    "    when applied to 1 of the bandit problems. This makes up 1 run. Repeating this for 2000 independent runs, each with a different \n",
    "    bandit problem, we obtained measures of the learning algorithm’s average behavior.\n",
    "    :param runs: Number of runs\n",
    "    :param times: Number of times\n",
    "    :param bandits: Bandit problems\n",
    "    :return: Optimal action count mean and reward mean\n",
    "    \"\"\"\n",
    "    # endregion Summary\n",
    "    \n",
    "    # region Body\n",
    "    \n",
    "    # Prepare a matrix filled with 0s for rewards\n",
    "    rewards = np.zeros((len(bandits),runs, times))\n",
    "    \n",
    "    # Prepare a matrix filled with 0s for optimal action counts that has the same shape as rewards matrix\n",
    "    optimal_action_counts = np.zeros(rewards.shape)\n",
    "\n",
    "    # For every bandit\n",
    "    for i, bandit in enumerate(bandits):\n",
    "        # for every run\n",
    "        for run in trange(runs):\n",
    "            # initialize bandit\n",
    "            bandit.initialize()\n",
    "            \n",
    "            # for every time step\n",
    "            for time in range(times):\n",
    "                # select an action\n",
    "                action = bandit.act()\n",
    "                \n",
    "                # get the reward\n",
    "                rewards[i, run, time] = bandit.step(action)\n",
    "                \n",
    "                # if the selected action is optimal for bandit\n",
    "                if action == bandit.optimal_action:\n",
    "                    # change the corresponding 0 in the optimal action counts matrix to 1\n",
    "                    optimal_action_counts[i, run, time] = 1\n",
    "\n",
    "    return optimal_action_counts.mean(axis=1), rewards.mean(axis=1)\n",
    "\n",
    "    # endregion Body"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-02-24T10:12:39.722889Z",
     "start_time": "2025-02-24T10:12:39.716431Z"
    }
   },
   "id": "be09fd89ebd40d84"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 1. Reward Distribution"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4088366f60e51478"
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "outputs": [],
   "source": [
    "# Plot an example reward distribution\n",
    "plt.violinplot(dataset=np.random.randn(200, 10) + np.random.randn(10))\n",
    "plt.title(\"Figure 2.1\")\n",
    "plt.xlabel(\"Action\")\n",
    "plt.ylabel(\"Reward distribution\")\n",
    "plt.savefig(\"../generated_images/figure_2_1.png\")\n",
    "plt.close()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-02-24T10:12:40.585021Z",
     "start_time": "2025-02-24T10:12:40.507842Z"
    }
   },
   "id": "8ed1daafa4064440"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 2. Greedy Action Selection VS ε-greedy Action Selection"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ef67eb7574c5d2b1"
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "outputs": [],
   "source": [
    "# Create a list of epsilons with 0, 0.1 and 0.01 values\n",
    "epsilons = [0, 0.1, 0.01]\n",
    "\n",
    "# Create a list of bandits (1 bandit for every epsilon) where every bandit uses sample-average method\n",
    "bandits = [Bandit(epsilon=epsilon, use_sample_averages=True) for epsilon in epsilons]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-02-24T10:12:41.402765Z",
     "start_time": "2025-02-24T10:12:41.399114Z"
    }
   },
   "id": "6a180bc790c31e65"
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2000/2000 [00:18<00:00, 105.81it/s]\n",
      "100%|██████████| 2000/2000 [00:18<00:00, 110.17it/s]\n",
      "100%|██████████| 2000/2000 [00:18<00:00, 108.79it/s]\n"
     ]
    }
   ],
   "source": [
    "# Define number of runs\n",
    "runs = 2000\n",
    "\n",
    "# Define number of times\n",
    "time = 1000\n",
    "\n",
    "# Simulate optimal action counts and rewards\n",
    "optimal_action_counts, rewards = simulate(runs, time, bandits)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-02-24T10:13:37.228977Z",
     "start_time": "2025-02-24T10:12:41.766354Z"
    }
   },
   "id": "683805477a8d4606"
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "outputs": [
    {
     "data": {
      "text/plain": "<Figure size 1000x2000 with 0 Axes>"
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Plotting\n",
    "plt.figure(figsize=(10, 20))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-02-24T10:14:18.114646Z",
     "start_time": "2025-02-24T10:14:18.107488Z"
    }
   },
   "id": "e1a86ca5f4aefa2"
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "outputs": [
    {
     "data": {
      "text/plain": "<matplotlib.legend.Legend at 0x2aeb6da6b10>"
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "plt.subplot(2, 1, 1)\n",
    "for epsilon, rewards in zip(epsilons, rewards):\n",
    "    plt.plot(rewards, label=\"$\\epsilon = %.02f$\" % epsilon)\n",
    "plt.title(\"Figure 2.2\")\n",
    "plt.xlabel(\"Steps\")\n",
    "plt.ylabel(\"Average reward\")\n",
    "plt.legend()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-02-24T10:14:19.670864Z",
     "start_time": "2025-02-24T10:14:19.656325Z"
    }
   },
   "id": "5536109f4e591e72"
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "outputs": [
    {
     "data": {
      "text/plain": "<matplotlib.legend.Legend at 0x2aeb6db38d0>"
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "plt.subplot(2, 1, 2)\n",
    "for epsilon, counts in zip(epsilons, optimal_action_counts):\n",
    "    plt.plot(counts, label=\"$\\epsilon = %.02f$\" % epsilon)\n",
    "plt.xlabel(\"Steps\")\n",
    "plt.ylabel(\"% Optimal action\")\n",
    "plt.legend()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-02-24T10:14:20.657930Z",
     "start_time": "2025-02-24T10:14:20.644537Z"
    }
   },
   "id": "2e6157d53f01223f"
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "outputs": [],
   "source": [
    "plt.savefig(\"../generated_images/figure_2_2.png\")\n",
    "plt.close()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-02-24T10:14:22.221273Z",
     "start_time": "2025-02-24T10:14:21.972153Z"
    }
   },
   "id": "ca9dfed4b31f4579"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 3. Optimistic Initial Values VS Realistic Initial Values"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a0c5945f58dd0dee"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Create a list of 2 bandits where:\n",
    "# 1. 1st bandit: ε = 0, 𝑄_1(𝑎) = 5, 𝛼 = 0.1,\n",
    "# 2. 2nd bandit: ε = 0.1, 𝑄_1(𝑎) = 0, 𝛼 = 0.1\n"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "id": "50d647979ced258a"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Define number of runs\n",
    "\n",
    "\n",
    "# Define number of times\n",
    "\n",
    "\n",
    "# Simulate optimal action counts\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3116e78a4c90c435"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Plotting\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d1ae633f8632eed5"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 4. Upper-Confidence-Bound (UCB) Action Selection"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7473708c239f1d0"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Create a list of 2 bandits where:\n",
    "# 1. 1st bandit: ε = 0, 𝑐 = 2, uses sample-average method,\n",
    "# 2. 2nd bandit: ε = 0.1, uses sample-average method\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1993531b4fe5feb2"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Define number of runs\n",
    "\n",
    "\n",
    "# Define number of times\n",
    "\n",
    "\n",
    "# Simulate average rewards\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6e1fed28f6812c2e"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Plotting\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9d4db60f0153c024"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 5. Gradient Bandit Algorithms (GBA)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b5cb31b7d224bbba"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Create a list of 4 bandits where:\n",
    "# 1. 1st bandit: uses GBA, 𝛼 = 0.1, uses average reward as baseline for GBA, expects true reward of 4,\n",
    "# 2. 2nd bandit: uses GBA, 𝛼 = 0.1, doesn't use average reward as baseline for GBA, expects true reward of 4,\n",
    "# 3. 3rd bandit: uses GBA, 𝛼 = 0.4, uses average reward as baseline for GBA, expects true reward of 4,\n",
    "# 4. 4th bandit: uses GBA, 𝛼 = 0.4, doesn't use average reward as baseline for GBA, expects true reward of 4\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1453e8fb0e6a32f6"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Define number of runs\n",
    "\n",
    "\n",
    "# Define number of times\n",
    "\n",
    "\n",
    "# Simulate optimal action counts\\\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "79a2acb7e523f0a8"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Labels\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "67282242fae58cb9"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Plotting\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2281e1a4dc8f1b9c"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "974417449ca9770c"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
