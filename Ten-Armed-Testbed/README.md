# **Multi-Armed Bandit: Ten-Armed Testbed**  

This project models the **k-armed bandit problem**, exploring various action selection strategies to optimize reward learning. The implementation includes flexible exploration methods, incremental action-value estimation, and high-resolution visualizations.

## **üìñ References**  

This repository is inspired by:  

- **Reinforcement Learning: An Introduction**  
  **Richard S. Sutton & Andrew G. Barto**  
  _Second Edition, MIT Press, 2018_  
  [Read more](https://www.andrew.cmu.edu/course/10-703/textbook/BartoSutton.pdf)  

---  

## **üìÇ Project Structure**  
```
Ten-Armed-Testbed/
‚îÇ‚îÄ‚îÄ src/                      # Core implementation
‚îÇ   ‚îú‚îÄ‚îÄ bandit.py             
‚îÇ   ‚îú‚îÄ‚îÄ __init__.py  
‚îÇ‚îÄ‚îÄ notebooks/                # Jupyter Notebooks for experimentation
‚îÇ   ‚îú‚îÄ‚îÄ ten_armed_testbed.ipynb
‚îÇ‚îÄ‚îÄ generated_images/         # Saved plots from simulations
‚îÇ   ‚îú‚îÄ‚îÄ figure_2_1.png
‚îÇ   ‚îú‚îÄ‚îÄ figure_2_2.png
‚îÇ   ‚îú‚îÄ‚îÄ figure_2_3.png
‚îÇ   ‚îú‚îÄ‚îÄ figure_2_4.png
‚îÇ   ‚îú‚îÄ‚îÄ figure_2_5.png
‚îÇ   ‚îú‚îÄ‚îÄ figure_2_5_10000_steps.png
‚îÇ‚îÄ‚îÄ requirements.txt          # Dependencies
‚îÇ‚îÄ‚îÄ README.md                 # Project documentation
```

---

## **üöÄ Getting Started**  

### 1Ô∏è‚É£ **Clone the Repository**  
```sh
git clone https://github.com/LilithMnatsakanyan/Ten-Armed-Testbed.git  
cd Ten-Armed-Testbed
```
### 2Ô∏è‚É£ **Install Dependencies**
```sh
pip install -r requirements.txt  
```
### Ô∏è3Ô∏è‚É£ **Run and experiment with Jupyter Notebooks**

```sh
notebooks/ten_armed_testbed.ipynb
```

---

## üìå Key Features

‚úÖ Models the **k-armed bandit problem**, simulating environments with multiple actions of unknown reward distributions.  
‚úÖ Provides **flexible exploration strategies**, including Œµ-greedy, Upper-Confidence-Bound (UCB), and gradient bandit methods.  
‚úÖ Supports **sample-average** and **constant step-size** approaches for **incremental action-value estimation**.  
‚úÖ Generates **high-resolution plots** illustrating learning performance, including **average rewards** and **optimal action selection rates** over time.  
‚úÖ Implements **optimistic initial values** to encourage exploration and **UCB-based action selection** for adaptive exploration.  

---

## üìä Results and Insights

### 1Ô∏è‚É£ Reward Distribution
In the **k-armed bandit problem**, each action has an **unknown true reward**, modeled as a stationary distribution. The optimal strategy should learn these reward values over time.

- Each bandit's reward distribution is **independent** and follows a **normal pattern**.
- The agent **estimates action values** and refines them based on observed rewards.
- Different learning strategies affect **how quickly and accurately** the optimal action is identified.

### üìà **Plot:** 

![figure_2_1.png](generated_images%2Ffigure_2_1.png)

---

### 2Ô∏è‚É£ Greedy Action Selection VS Œµ-greedy Action Selection

**Exploration vs. Exploitation:**  
- **Greedy selection (Œµ = 0.00):** Always selects the action with the highest estimated value but fails in stochastic environments due to lack of exploration.  
- **Œµ-greedy selection (Œµ > 0.00):** With probability Œµ, a random action is chosen, ensuring occasional exploration.  
- **Higher Œµ (Œµ = 0.10):** Encourages exploration early, finding better actions faster but sacrificing long-term stability.  
- **Lower Œµ (Œµ = 0.01):** Balances exploration and exploitation, leading to better long-term convergence.  

‚úÖ **Optimal Strategy:** A small, nonzero Œµ (e.g., **Œµ = 0.01**) achieves the best balance between exploration and long-term performance.

### üìà **Plot:** 

![figure_2_2.png](generated_images%2Ffigure_2_2.png)

---

### 3Ô∏è‚É£ Optimistic Initial Values VS Realistic Initial Values
A common way to encourage exploration **without Œµ-greedy randomness** is using **optimistic initial estimates** for action values.

- **Optimistic Initial Values:** Assumes **higher-than-expected** rewards, forcing early exploration.
- **Realistic Initial Values:** Starts conservatively, relying on **Œµ-greedy or UCB** for exploration.
- **Key Insight:** In **stationary environments**, optimistic initial values promote better early exploration but **converge similarly** to Œµ-greedy over time.
- **Limitation:** In **non-stationary environments**, optimistic initialization is ineffective because the agent stops exploring once high estimates are corrected.

### üìà **Plot:** 

![figure_2_3.png](generated_images%2Ffigure_2_3.png)

---

### 4Ô∏è‚É£ Upper-Confidence-Bound (UCB) Action Selection
Instead of exploring randomly, **UCB-based methods** select actions based on their **estimated value** and **uncertainty**.

#### **Key Observations:**
- **UCB prioritizes exploration based on confidence intervals**, reducing unnecessary exploration of suboptimal actions.
- **In stationary environments,** UCB **outperforms Œµ-greedy** by selecting actions **more systematically**.
- **In non-stationary environments,** UCB may struggle if action values change over time.
- The **exploration term** ensures actions with fewer selections are revisited more frequently early on.

‚úÖ **Conclusion:** UCB **efficiently balances exploration and exploitation**, making it a superior choice in many cases.

### üìà **Plot:** 

![figure_2_4.png](generated_images%2Ffigure_2_4.png)

---

### 5Ô∏è‚É£ Gradient Bandit Algorithms (GBA)
Unlike value-based methods, **Gradient Bandit Algorithms** focus on learning **preferences** rather than explicit action values.

### üìà **Plot:** 

![figure_2_5.png](generated_images%2Ffigure_2_5.png)

#### **Key Insights:**
- **Policy-Based Approach:** Uses **softmax probabilities** instead of maintaining action-value estimates.
- **With Reward Baseline:** More **stable learning**, adjusting preferences based on relative rather than absolute rewards.
- **Without Reward Baseline:** Slower convergence, as the agent lacks a reference for reward comparison.
- **Performs well in non-stationary environments**, where traditional value-estimation methods struggle.

‚úÖ **Conclusion:**  Gradient Bandit methods are especially effective when rewards depend on probability distributions rather than fixed values. Initially, I assumed that a 10,000-step plot would confirm that the difference between 0.1 with baseline and without baseline would diminish over time. However, after running the experiment, I realized that while this trend holds, continuing to experiment beyond this point provides little additional insight. Additionally, a 10,000-step plot is included, revealing that the difference between 0.1 with baseline and without baseline becomes negligible over longer runs, making further experimentation unnecessary.

### üìà **Plot:** 

![figure_2_5_10000_steps.png](generated_images%2Ffigure_2_5_10000_steps.png)

---

## üì¢ Conclusion

This project explores **several action selection strategies** for the **k-armed bandit problem**, comparing their effectiveness in balancing **exploration and exploitation**.

- **Œµ-Greedy ensures exploration** but is **random** and inefficient.
- **Optimistic Initial Values encourage early exploration** but fail in non-stationary settings.
- **UCB dynamically adjusts exploration**, outperforming Œµ-greedy in many cases.
- **Gradient Bandit methods** are useful when **probability-based action selection** is required.

By **analyzing reward distributions, action selection methods, and long-term performance**, this project highlights the strengths and weaknesses of different **reinforcement learning strategies** in decision-making problems.

---

