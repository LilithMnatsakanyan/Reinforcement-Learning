# **Gridworld DP**

This project implements **Dynamic Programming (DP)** methods in the **Gridworld** environment, based on **Chapter 4: Dynamic Programming** from the book **Reinforcement Learning: An Introduction** by **Richard S. Sutton & Andrew G. Barto**.

## **ğŸ“– References**
This repository is based on:

- **Reinforcement Learning: An Introduction**  
  **Richard S. Sutton & Andrew G. Barto**  
  _Second Edition, MIT Press, 2018_  
  [Read more](https://www.andrew.cmu.edu/course/10-703/textbook/BartoSutton.pdf)

---

## **ğŸ“‚ Project Structure**
```
gridworld-dp/
â”‚â”€â”€ src/                      # Core implementation
â”‚   â”œâ”€â”€ __init__.py  
â”‚   â”œâ”€â”€ grid_world.py         # Gridworld DP logic
â”‚â”€â”€ notebooks/                # Jupyter Notebooks for experimentation
â”‚   â”œâ”€â”€ grid_world.ipynb      
â”‚â”€â”€ book_images/              # Reference images from the book
â”‚   â”œâ”€â”€ Example_4_1.PNG       # Policy iteration example (book version)
â”‚   â”œâ”€â”€ Figure_4_1.PNG        # Gridworld configuration and value function example
â”‚â”€â”€ generated_images/         # Plots generated from simulations
â”‚   â”œâ”€â”€ figure_4_1_in_place.png      # In-place policy evaluation results
â”‚   â”œâ”€â”€ figure_4_1_out_place.png     # Out-of-place policy evaluation results
â”‚â”€â”€ README.md                 # Project documentation
```

---

## **ğŸš€ Getting Started**

### 1ï¸âƒ£ **Clone the Repository**
```sh
git clone https://github.com/LilithMnatsakanyan/gridworld-dp.git  
cd gridworld-dp
```

### 2ï¸âƒ£ **Install Dependencies**
```sh
pip install -r requirements.txt  
```

### ï¸3ï¸âƒ£ **Run and experiment with Jupyter Notebooks**
```sh
jupyter notebook notebooks/grid_world.ipynb
```

---

## ğŸ“Œ Key Features

âœ… Implements **Dynamic Programming** for policy evaluation and improvement in Gridworld.  
âœ… Demonstrates **in-place vs. out-of-place** updates.  
âœ… Visualizes the **value function** and **policy iteration process**.  
âœ… Clean separation between logic and experiments using notebooks.  
âœ… Great for building foundational understanding of RL.

---

## ğŸ“Š Results and Visualizations

### 1ï¸âƒ£ **In-Place Policy Evaluation**
This approach updates the value of each state as soon as a new estimate is available.

ğŸ“ˆ **Visualization:**

![figure_4_1_in_place.png](generated_images/figure_4_1_in_place.png)

(This plot shows the value function computed using in-place dynamic programming.)

---

### 2ï¸âƒ£ **Out-of-Place Policy Evaluation**
This approach computes the entire new value function based on the old one before replacing it.

ğŸ“ˆ **Visualization:**

![figure_4_1_out_place.png](generated_images/figure_4_1_out_place.png)

(This plot illustrates the value function computed using out-of-place updates.)

---

## ğŸ“¢ Conclusion

This project explores **Dynamic Programming techniques** applied to the **Gridworld** environment:

- **In-place and out-of-place updates** for policy evaluation.  
- Understanding **policy iteration** and **value iteration**.  
- Lays the groundwork for **model-based reinforcement learning**.

Through these techniques, we gain an intuition for how agents can plan and make decisions in structured environments.

---

