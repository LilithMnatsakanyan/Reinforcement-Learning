# **Reinforcement Learning: Maximization Bias**

This project implements **Q-learning** and **Double Q-learning** to demonstrate the problem of **maximization bias** in action-value methods. It is based on **Chapter 6: Temporal-Difference Learning**, specifically **Example 6.5**, from the book *Reinforcement Learning: An Introduction* by **Richard S. Sutton & Andrew G. Barto**.

---

## **ğŸ“‚ Project Structure**

```
maximization-bias/
â”œâ”€â”€ src/                          # Core implementation
â”‚   â””â”€â”€ maximization_bias.py      # Logic for Q-learning and Double Q-learning
â”œâ”€â”€ notebooks/                    # Jupyter Notebook for experimentation
â”‚   â””â”€â”€ maximization_bias.ipynb
â”œâ”€â”€ book_images/                  # Reference figures from the book
â”‚   â”œâ”€â”€ Figure_6_5_graph.PNG
â”‚   â””â”€â”€ Figure_6_5_mdp.PNG
â”œâ”€â”€ generated_images/             # Plots generated from simulations
â”‚   â””â”€â”€ figure_6_5.png
â””â”€â”€ README.md                     # Project documentation
```


---

## ğŸ“Œ Key Features
âœ… Implements **Q-learning** and **Double Q-learning**  
âœ… Demonstrates the **maximization bias problem** in value estimation  
âœ… Uses a simple **two-state MDP** to highlight overestimation effects  
âœ… Tracks the frequency of â€œleftâ€ actions in **state A** as a performance measure  
âœ… Reproduces Suttonâ€™s **Figure 6.5** experiment

---

## âš™ï¸ **Environment Overview**

- The environment is a **two-state MDP** with the following setup:

    - **States**:
        - `A` (start state)
        - `B` (intermediate)
        - `terminal` (absorbing state)

    - **Actions**:
        - From `A`: two actions â†’ `right` (to terminal) or `left` (to B)
        - From `B`: ten actions, all equivalent, leading to terminal

    - **Rewards**:
        - From `A`: always **0**
        - From `B`: reward sampled from **Normal(Î¼=-0.1, Ïƒ=1.0)**

---

## ğŸ§  **Learning Algorithms**

### ğŸ”· Q-learning
- Standard off-policy **action-value learning**.
- Updates based on the maximum estimated value at the next state.
- Suffers from **maximization bias** (systematic overestimation).

### ğŸ”´ Double Q-learning
- Maintains two separate action-value estimates (**Qâ‚, Qâ‚‚**).
- At each step, randomly chooses which one to update.
- Reduces maximization bias by **decoupling action selection and evaluation**.

---

## ğŸ“Š Results and Visualizations

### 1ï¸âƒ£ **Figures from Sutton's Book**
These figures illustrate the theoretical expectation of maximization bias:

ğŸ“ˆ **Visualization:**

<img src="book_images/Figure_6_5_mdp.PNG" alt="Sutton MDP Example" width="400"/>  
<img src="book_images/Figure_6_5_graph.PNG" alt="Sutton Fig 6.5" width="400"/>  

---

### 2ï¸âƒ£ **Generated Simulation Results**
The following results are produced by this implementation:

ğŸ“ˆ **Visualization:**

<img src="generated_images/figure_6_5.png" alt="Q-learning vs Double Q-learning" width="400"/>  

This shows how **Q-learning consistently favors the â€œleftâ€ action** in state A (due to maximization bias), whereas **Double Q-learning remains unbiased**.

---

## ğŸ” **Key Observations**
- **Q-learning** overestimates action values, leading to a systematic preference for risky choices.
- **Double Q-learning** corrects this bias and learns a more balanced policy.
- The experiment highlights why overestimation is problematic in reinforcement learning and how **Double Q-learning mitigates it**.

---

## ğŸ“¢ Conclusion
This project provides a hands-on demonstration of:

- **Maximization bias** in Q-learning
- How **Double Q-learning** addresses this issue
- Reproducing **Suttonâ€™s Example 6.5** as a classic benchmark

The Maximization Bias problem is a key example for understanding the **limitations of greedy updates** and the importance of unbiased estimators in RL.  
