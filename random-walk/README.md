
# **Reinforcement Learning: Random Walk**

This project implements **Monte Carlo (MC)** and **Temporal-Difference (TD(0))** learning for solving the **Random Walk** problem, based on **Chapter 6: Temporal-Difference Learning**, specifically **Example 6.2**, from the book **Reinforcement Learning: An Introduction** by **Richard S. Sutton & Andrew G. Barto**.


---

## **ğŸ“‚ Project Structure**
```
random_walk/
â”œâ”€â”€ src/                        # Core implementation
â”‚   â””â”€â”€ random_walk.py          # Logic for MC, TD(0), and batch RMSE evaluation
â”œâ”€â”€ notebooks/                  # Jupyter Notebook for experimentation
â”‚   â””â”€â”€ random_walk.ipynb
â”œâ”€â”€ book_images/                # Reference images from the book
â”‚   â”œâ”€â”€ Example_6_2_top.PNG
â”‚   â”œâ”€â”€ Example_6_2_bottom.PNG
â”‚   â””â”€â”€ Figure_6_2.PNG
â”œâ”€â”€ generated_images/           # Plots generated from simulations
â”‚   â”œâ”€â”€ example_6_2.png
â”‚   â””â”€â”€ figure_6_2.png
â””â”€â”€ README.md                   # Project documentation
```

---

## ğŸ“Œ Key Features
âœ… Implements **Monte Carlo** and **Temporal-Difference (TD(0))** learning algorithms  
âœ… Supports **batch mode** updates until convergence  
âœ… Measures **RMSE** to compare learning methods against true values  
âœ… Reproduces Suttonâ€™s **Figure 6.2** experiment with high fidelity  
âœ… Modular implementation suitable for further RL experimentation

---
## âš™ï¸ **Environment Overview**

- A **1D grid world** with **7 states**:
  - Terminal: `State 0 (Left), State 6 (Right)`
  - Non-terminal: `States 1â€“5 (Aâ€“E)`
- The agent starts from **State 3 (C)**.
- Each step, the agent randomly moves **left or right** with equal probability (p=0.5).
- The episode ends when the agent reaches a terminal state.
  - `State 6` gives reward **+1**
  - `State 0` gives reward **0**

### âœ… **True State Values**
These represent the ground truth:
```
V(A)=1/6, V(B)=2/6, V(C)=3/6, V(D)=4/6, V(E)=5/6
```

---

## ğŸ§  **Learning Algorithms**

### ğŸ”· Monte Carlo Prediction (MC)
- Updates state-values **after the episode ends**
- Uses the full return (final reward) for updates
- Equation:
  ```
  V(S) â† V(S) + Î± * (G - V(S))
  ```

### ğŸ”´ Temporal-Difference Learning (TD(0))
- Updates **during the episode**, step-by-step
- Uses **bootstrapping** (estimates next state's value)
- Equation:
  ```
  V(S) â† V(S) + Î± * (R + V(S') - V(S))
  ```

### âš« Batch Mode
- Collects multiple episodes before updating
- Iteratively updates all visited states from stored episodes until total change in values falls below a defined threshold (Î¸)
- Used to compare **TD vs MC** learning behavior on identical data

---
## ğŸ“Š Results and Visualizations

### 1ï¸âƒ£ **Figures from Sutton's Book**
These figures illustrate the expected value predictions and comparisons:

ğŸ“ˆ **Visualization:**

<img src="book_images/Figure_6_2.PNG" alt="Sutton Fig 6.2" width="400"/>
<img src="book_images/Example_6_2_top.PNG" alt="Sutton Example Top" width="400"/>
<img src="book_images/Example_6_2_bottom.PNG" alt="Sutton Example Bottom" width="400"/>

_These serve as theoretical reference baselines._

---

### 2ï¸âƒ£ **Generated Simulation Results**
The following are results of simulated MC and TD(0) learning:

ğŸ“ˆ **Visualization:**

<img src="generated_images/figure_6_2.png" alt="TD vs MC RMSE" width="400"/>
<img src="generated_images/example_6_2.png" alt="Value Comparison" width="400"/>

These show how both methods evolve value predictions and their accuracy relative to ground truth.

---

## ğŸ” **Key Observations**

- **TD(0)** converges faster due to bootstrapping, offering lower variance but potential bias.
- **Monte Carlo** produces unbiased value estimates with higher variance.
- In **batch mode**, TD and MC converge to different fixed pointsâ€”mirroring Suttonâ€™s original Figure 6.2.
- This confirms key RL concepts such as the **bias-variance trade-off** in value estimation.

---

## ğŸ“¢ Conclusion

This project offers a hands-on implementation of:

- **Monte Carlo** and **TD(0)** learning for policy evaluation
- Analyzing learning speed, stability, and estimation accuracy
- Replicating classic results from Sutton & Bartoâ€™s textbook

The Random Walk problem is an excellent first step for understanding **value prediction** using **model-free reinforcement learning**.

---
