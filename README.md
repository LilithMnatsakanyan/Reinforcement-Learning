# **Reinforcement Learning Projects**

This repository contains a collection of hands-on **Reinforcement Learning (RL)** projects, each focused on implementing and analyzing different algorithms and environments from the textbook **_Reinforcement Learning: An Introduction_** by **Richard S. Sutton** and **Andrew G. Barto**.

The goal of this repository is to develop a deeper understanding of:
- âœ… Agent learning and policy optimization
- âœ… Environment dynamics and reward mechanisms
- âœ… Explorationâ€“exploitation tradeoffs
- âœ… On-policy vs. off-policy learning

---

## ğŸ§  **Projects Overview**

Each project explores a specific RL concept or algorithm and includes its own `README.md` with:
- **Objectives** â€“ What the project aims to demonstrate
- **Methodology** â€“ Core algorithms and environment setup
- **Key Results** â€“ Visualizations, learning curves, and interpretations

---

## ğŸ“‚ **Current Projects**

### ğŸ§© **Foundations and Value-Based Learning**
- [tic-tac-toe](./tic-tac-toe/) â€“ Self-play using state-value learning
- [ten-armed-testbed](./ten-armed-testbed/) â€“ Multi-armed bandits with Îµ-greedy strategy

### ğŸ§® **Dynamic Programming and Model-Based Control**
- [gridworld-dp](./gridworld-dp/) â€“ Policy evaluation and improvement using Dynamic Programming
- [gridworld-mdp](./gridworld-mdp/) â€“ MDP formulation and planning
- [gambler-problem](./gambler-problem/) â€“ Dynamic programming for optimal betting

### ğŸ² **Monte Carlo and Temporal-Difference Methods**
- [blackjack](./blackjack/) â€“ Policy evaluation using Monte Carlo
- [random-walk](./random-walk/) â€“ TD(0) vs Monte Carlo prediction
- [random-walk-ntd](./random-walk-ntd/) â€“ n-step TD learning and return estimation

### âš™ï¸ **Control and Exploration**
- [windy-gridworld](./windy-gridworld/) â€“ SARSA learning in a stochastic grid environment
- [cliff-walking](./cliff-walking/) â€“ Episodic control with Îµ-greedy and SARSA/Q-learning
- [maximization-bias](./maximization-bias/) â€“ Demonstrating maximization bias with Q-learning vs Double Q-learning

### ğŸ§  **Planning and Model-Based Reinforcement Learning**
- [infinite-variance](./infinite-variance/) â€“ Off-policy learning and variance analysis
- [mazes](./mazes/) â€“ Dyna-Q, Dyna-Q+, and Prioritized Sweeping (planning and learning integration)
- [updates-comparison](./updates-comparison/) â€“ Expected vs sample updates in planning
- [trajectory-sampling](./trajectory-sampling/) â€“ Uniform vs on-policy update distributions

### ğŸ”¢ **Function Approximation and Generalization**
- [random-walk-fa](./random-walk-fa/) â€“ Function approximation methods (polynomial, Fourier, tile coding)
- [coarse-coding](./coarse-coding/) â€“ Coarse coding and generalization in value function approximation


---

## **ğŸš€ Getting Started** 
_(Example on tic-tac-toe project)_

### 1ï¸âƒ£ **Clone the Repository**
```sh
git clone https://github.com/LilithMnatsakanyan/tic-tac-toe.git  
cd tic-tac-toe
```

### 2ï¸âƒ£ **Install Dependencies**
```sh
pip install -r requirements.txt  
```

### 3ï¸âƒ£ **Train the RL Agent**
```sh
python src/tic-tac-toe.py  
```

### 4ï¸âƒ£ **Play the Game**

You can play Tic Tac Toe in two ways:

âœ… **Web-based interface** (Recommended):
```sh
python app.py  
```
Then open `index.html` in your browser to play against the AI.

âœ… **Console-based interface** (Using keyboard letters for a 3Ã—3 grid):
```sh
python src/tic-tac-toe.py  
```

---

## ğŸ“Œ **Key Topics Covered**

### ğŸ¯ **Foundational Algorithms**
- Multi-Armed Bandits (Action-Value Methods)
- Dynamic Programming (Policy Evaluation & Improvement)
- Monte Carlo Prediction and Control
- Temporal-Difference Learning (TD(0), n-step TD, SARSA, Q-Learning)
- Planning Methods (Dyna-Q, Prioritized Sweeping)
- Off-policy Learning and Variance (Importance Sampling)
- Function Approximation (Linear, Fourier, Tile Coding, Coarse Coding)
- Double Q-Learning

### ğŸ§ª **Experimental Environments**
- Tic-Tac-Toe
- Multi-Armed Bandits (Ten-Armed Testbed)
- Gridworld (DP and MDP Variants)
- Gamblerâ€™s Problem
- Blackjack
- Random Walk (1D and n-step Variants)
- Windy Gridworld
- Cliff Walking
- Infinite Variance
- Mazes (Dyna-Q and Prioritized Sweeping)
- Updates Comparison
- Trajectory Sampling
- Random Walk with Function Approximation
- Square Wave (Coarse Coding / Feature Width Analysis)
- Maximization Bias

---

## **ğŸ“š Dependencies & Tools**

This repository primarily uses:
- **Python 3.x**
- **NumPy, Pandas** (Data handling)
- **Matplotlib, Seaborn** (Visualization)
- **tqdm** (Progress bars for episode tracking)


---

## ğŸ“– **Reference**

> **Reinforcement Learning: An Introduction**  
> Richard S. Sutton & Andrew G. Barto  
> _Second Edition, 2018 â€“ MIT Press_  
> [Read online](https://www.andrew.cmu.edu/course/10-703/textbook/BartoSutton.pdf)

---

## ğŸ“¬ **Contact**

If you have any questions, feel free to reach out! ğŸ˜Š

ğŸ”— GitHub: [github.com/LilithMnatsakanyan](https://github.com/LilithMnatsakanyan)

---
