# **Reinforcement Learning: Windy Gridworld**

This project implements the **SARSA (State-Action-Reward-State-Action)** algorithm to solve the **Windy Gridworld** problem, as presented in **Chapter 6: Temporal-Difference Learning**, specifically **Example 6.5**, from the book **Reinforcement Learning: An Introduction** by **Richard S. Sutton & Andrew G. Barto**.


---

## **ğŸ“‚ Project Structure**
```
windy-gridworld/
â”œâ”€â”€ src/                            # Core implementation
â”‚   â””â”€â”€ windy_grid_world.py         # Windy Gridworld logic and SARSA learning
â”œâ”€â”€ notebooks/                      # Jupyter Notebook for running experiments
â”‚   â””â”€â”€ windy_grid_world.ipynb
â”œâ”€â”€ book_images/                    # Reference images from the book
â”‚   â”œâ”€â”€ Example_6_5_graph.PNG
â”‚   â””â”€â”€ Example_6_5_inset.PNG
â”œâ”€â”€ generated_images/               # Plots generated from experiments
â”‚   â””â”€â”€ example_6_5.png
â””â”€â”€ README.md                       # Project documentation
```

---

## ğŸ“Œ Key Features
âœ… Implements **SARSA** for learning action-value estimates  
âœ… Uses **Îµ-greedy** strategy for action selection  
âœ… Handles **wind** dynamics in a gridworld environment  
âœ… Tracks number of steps per episode until reaching goal  
âœ… Reproduces Suttonâ€™s **Example 6.5** setting with adjustable parameters

---

## âš™ï¸ **Environment Overview**

- A **7Ã—10 grid world** with wind affecting vertical movement:
    - The **wind strength** varies per column and pushes the agent **upward**.
    - Stronger wind moves the agent higher in the grid.
- The agent starts at **(3, 0)** and aims to reach the goal at **(3, 7)**.
- **Available actions**: up, down, left, right
- **Wind strength** by column:
  ```
  [0, 0, 0, 1, 1, 1, 2, 2, 1, 0]
  ```
- Every step gives a reward of **âˆ’1**, encouraging faster paths.

---

## ğŸ§  **Learning Algorithm**

### ğŸ”· SARSA (On-policy TD Control)
- Updates **action-value estimates** based on actual policy followed.
- Learns **Q(s, a)** values while following an Îµ-greedy policy.
- **Update Rule (Equation 6.7)**:
  ```
  Q(S, A) â† Q(S, A) + Î± * [R + Î³ * Q(S', A') âˆ’ Q(S, A)]
  ```
- Incorporates:
    - **Exploration probability (Îµ):** 0.1
    - **Step size (Î±):** 0.5
    - **Discount factor (Î³):** 1.0

---

## ğŸ“Š Results and Visualizations

### 1ï¸âƒ£ **Figures from Sutton's Book**
Visual illustrations of the Windy Gridworld setup and results:

ğŸ“ˆ **Reference Visuals:**

<img src="book_images/Example_6_5_graph.PNG" alt="Example 6.5 Graph" width="400"/>
<img src="book_images/Example_6_5_inset.PNG" alt="Example 6.5 Inset" width="400"/>

---

### 2ï¸âƒ£ **Generated Results**
Simulated outcomes based on the current implementation:

ğŸ“ˆ **Simulation Output:**

<img src="generated_images/example_6_5.png" alt="SARSA Episode Steps" width="400"/>

This shows the **number of time steps per episode** as the agent learns to reach the goal more efficiently over time.

---

## ğŸ” **Key Observations**

- The agent learns to reach the goal more efficiently as episodes progress.
- Wind dynamics introduce additional complexity, requiring the agent to learn compensatory actions.
- The Îµ-greedy strategy ensures continued exploration to avoid local optima.

---

## ğŸ“¢ Conclusion

This project demonstrates:

- On-policy control using **SARSA**
- The influence of environmental dynamics (wind) on agent behavior
- Application of foundational **TD learning techniques** in a gridworld setting

Windy Gridworld serves as an essential example in understanding **action-value learning** and the balance between **exploration and exploitation** in reinforcement learning.

---
